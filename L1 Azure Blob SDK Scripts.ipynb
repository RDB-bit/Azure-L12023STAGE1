{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import shutil\n",
    "import urllib.parse\n",
    "\n",
    "import logging\n",
    "from logging.handlers import RotatingFileHandler\n",
    "\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Define log format\n",
    "log_file = 'blob_log.log' #I defined the log file before logger and the logger stopped working\n",
    "log_format = '%(asctime)s - %(levelname)s - %(name)s - %(module)s - %(lineno)d - %(message)s'\n",
    "formatter = logging.Formatter(log_format)\n",
    "\n",
    "if not logger.handlers:\n",
    "    # Define log file and handler\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.propagate = False\n",
    "    # logging.disable(logging.DEBUG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Local files to Blob Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Script to work your way around blob '''\n",
    "\n",
    "# The credentials are stored as environment variable using json\n",
    "# Using only BlobServiceClient for all container and blob interactions\n",
    "\n",
    "\n",
    "class AzureStorageManipulation:\n",
    "    \n",
    "\n",
    "    def __init__(self, json_path='var.json'):\n",
    "        \n",
    "        self.config_path = json_path\n",
    "        self.config_file = None\n",
    "        self.blob_service_client = None\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        \n",
    "    def load_config(self):\n",
    "\n",
    "        try:\n",
    "            with open(self.config_path, 'r') as file:\n",
    "                self.config_file = json.load(file)\n",
    "        except Exception as e:\n",
    "            self.logger.error(\"Failed to load configuration: %s\", e)\n",
    "            print(e)\n",
    "\n",
    "\n",
    "    def initialize_client(self):\n",
    "\n",
    "        try:\n",
    "            self.blob_service_client = BlobServiceClient.from_connection_string(conn_str=self.config_file[\"on_cloud\"][\"conn_string_BlobStorage\"])\n",
    "        except Exception as e:\n",
    "            self.logger.error(\"Failed to initialize blob service client: %s\", e)\n",
    "            print(e)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Child class for containers - the_container_wars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TheContainerWars(AzureStorageManipulation):\n",
    "\n",
    "    def __init__(self, json_path='var.json'):\n",
    "        super().__init__(json_path)\n",
    "\n",
    "\n",
    "\n",
    "    def check_status_container(self, container_name):\n",
    "\n",
    "        try:\n",
    "            container_client = self.blob_service_client.get_container_client(container_name)\n",
    "\n",
    "            if container_client.exists():\n",
    "                self.logger.info(f\"The container {container_name} exists.\")\n",
    "\n",
    "                # Check if the container is empty\n",
    "                count = 0\n",
    "                if sum((count+1 for blob in container_client.list_blobs())) > 0:\n",
    "\n",
    "                    self.logger.info(f\"The container {container_name} is not empty.\")\n",
    "                    return \"not_empty\"\n",
    "\n",
    "                else:\n",
    "                    self.logger.info(f\"The container {container_name} is empty.\")\n",
    "                    return \"empty\"      \n",
    "                    \n",
    "            else:\n",
    "                self.logger.info(f\"The container {container_name} does not exist.\")\n",
    "                return \"not_exists\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to check container {container_name} status due to error: {e}\")\n",
    "            return \"error\"\n",
    "\n",
    "    \n",
    "\n",
    "    def make_container(self, container_name):\n",
    "\n",
    "        if self.check_status_container(container_name) == \"not_exists\":\n",
    "            \n",
    "            try:\n",
    "                # Create new container in the service\n",
    "                container_client = self.blob_service_client.get_container_client(container_name)\n",
    "                container_client.create_container()\n",
    "                self.logger.info(f\"Success: container {container_name} created\")\n",
    "\n",
    "            # except AssertionError:\n",
    "                # self.logger.error(f\"Failed to create container {container_name} as it already exists\")\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Failed to create container {container_name} due to error: {e}\")\n",
    "\n",
    "        else:\n",
    "            self.logger.error(f\"Failed to create container {container_name} as it already exists\")\n",
    "\n",
    "\n",
    "\n",
    "    def list_containers(self):\n",
    "\n",
    "        '''Return an iterator'''\n",
    "\n",
    "        try:    \n",
    "            # List containers in the storage account\n",
    "            return self.blob_service_client.list_containers()\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to list containers due to error: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "    def copy_container_content(self, container_old, container_new, container_new_exists_check = True):\n",
    "\n",
    "        ''' Need to add checks in for container existance and emptiness'''\n",
    "\n",
    "        container_status = self.check_status_container(container_old)\n",
    "        \n",
    "        if container_status == \"not_exists\":\n",
    "            self.logger.error(f\"Failed to copy {container_old} contents as the container {container_old} does not exist. Returning None\")\n",
    "            return\n",
    "        elif container_status == \"empty\":\n",
    "            self.logger.error(f\"Failed to copy {container_old} contents as the container {container_old} is empty. Returning None\")\n",
    "            return\n",
    "        \n",
    "        if container_new_exists_check:\n",
    "            container_status = self.check_status_container(container_new)\n",
    "            \n",
    "            if container_status == \"not_exists\":\n",
    "                self.logger.error(f\"Failed to copy {container_old} contents as the container {container_new} does not exist. Returning None\")\n",
    "                return\n",
    "            \n",
    "        try:\n",
    "            old_container_client = self.blob_service_client.get_container_client(container_old)\n",
    "            blobs = old_container_client.list_blobs()\n",
    "\n",
    "            # Copy the blobs from the original container to the new container\n",
    "            new_container_client = self.blob_service_client.get_container_client(container_new)\n",
    "            \n",
    "            for blob in blobs:\n",
    "                old_blob_client = old_container_client.get_blob_client(blob.name)\n",
    "                new_blob_client = new_container_client.get_blob_client(blob.name)\n",
    "                new_blob_client.start_copy_from_url(old_blob_client.url)\n",
    "\n",
    "            self.logger.info(f\"Success: Blobs copied from container {container_old} to container {container_new}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to copy blobs from container {container_old} to container {container_new} : {e}\")\n",
    "\n",
    "\n",
    "\n",
    "    def rename_containers(self, name_old, name_new, container_new_exists_check = False):\n",
    "\n",
    "        # bit long winded\n",
    "        try:\n",
    "\n",
    "            assert self.check_status_container(name_new) == \"not_exists\"\n",
    "      \n",
    "            # make new container\n",
    "            self.make_container(name_new)\n",
    "            self.copy_container_content(name_old, name_new, container_new_exists_check)\n",
    "            self.delete_container(name_old)\n",
    "\n",
    "        except AssertionError:\n",
    "            self.logger.error(f\"Failed: Container with name {name_new} already exists\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to rename container {name_old} to {name_new}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "    def delete_container(self, container_name):\n",
    "\n",
    "        # Instantiate a client\n",
    "        container_client = self.blob_service_client.get_container_client(container_name)\n",
    "\n",
    "        try:\n",
    "            container_client.delete_container()\n",
    "            self.logger.info(f\"Success: container {container_name} successfully deleted\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to delete container: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Child class for blob - The blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TheBlob(AzureStorageManipulation):\n",
    "\n",
    "    def __init__(self, json_path='var.json'):\n",
    "        super().__init__(json_path)\n",
    "\n",
    "\n",
    "    def upload_blobs(self):\n",
    "\n",
    "        '''Uploads multiple blobs from local to cloud'''\n",
    "\n",
    "        try: \n",
    "            # get local folder name and files to upload\n",
    "            folder_local = self.config_file[\"on_prem\"][\"containerRawDataProcessed\"]\n",
    "            files = os.listdir(folder_local)\n",
    "            files_num = len(files)\n",
    "            count = 0\n",
    "\n",
    "            # get destination cloud folder\n",
    "            folder_cloud= self.config_file[\"on_cloud\"][\"containerRawDataProcessed\"]\n",
    "            container_client = self.blob_service_client.get_container_client(folder_cloud)\n",
    "\n",
    "            if container_client.exists():\n",
    "                # Upload local files, use local file name for the blob name\n",
    "                for file in files:\n",
    "\n",
    "                    # absolute path of the file \n",
    "                    filepath_local = os.path.join(folder_local, file)\n",
    "\n",
    "                    # create blob client for every blob\n",
    "                    client_blob = self.blob_service_client.get_blob_client(folder_cloud, file)\n",
    "\n",
    "                    # upload the file (blob)\n",
    "                    with open(filepath_local, \"rb\") as data:\n",
    "                        client_blob.upload_blob(data, overwrite=True)\n",
    "                        \n",
    "                    count = count+1\n",
    "\n",
    "                self.logger.info(f\"Success: 'Upload Blob' completed. Uploaded {count} of {files_num} files to container {folder_cloud}\\n\")\n",
    "\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed: 'Upload Blob' failed due to error {e}\\n\" )\n",
    "            \n",
    "\n",
    "\n",
    "    def delete_blob(self, container_name, blob_name):\n",
    "\n",
    "        try:\n",
    "\n",
    "            container_client = self.blob_service_client.get_container_client(container_name, blob_name)\n",
    "            blob_client = container_client.get_blob_client(blob_name)\n",
    "\n",
    "            blob_client.delete_blob()\n",
    "            self.logger.info(f\"Success: blob {blob_name} successfully deleted\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to delete blob {blob_name}: {e}\")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # initialize the parent class and load the configuration stored in json file\n",
    "    logger.info('START OF CONTAINER WARS')\n",
    "    config = TheContainerWars()\n",
    "    config.load_config() # lazy loading -> only done when needed\n",
    "    config.initialize_client() # A client to interact with the Blob Service at the account level\n",
    "    \n",
    "    logger.info('CREATE CONTAINER INSTANCE RUNNING')\n",
    "    config.make_container('testcontainer')\n",
    "\n",
    "    logger.info('LISTING CONTAINERS INSTANCE RUNNING')\n",
    "    containers = config.list_containers()\n",
    "    for container in containers:\n",
    "        print(container.name)\n",
    "\n",
    "    logger.info('COPYING CONTAINER CONTENT INSTANCE RUNNING')\n",
    "    config.copy_container_content('data-ready', 'testcontainer')\n",
    "    config.copy_container_content('data-ready', 'testnewcontainer')\n",
    "\n",
    "    logger.info('RENAMING CONTAINER INSTANCE RUNNING')\n",
    "    config.rename_containers('testcontainer', 'testnewcontainer')\n",
    "    config.rename_containers('testcontainer', 'testnewcontainer')\n",
    "\n",
    "    logger.info('DELETE CONTAINER INSTANCE RUNNING')\n",
    "    config.delete_container('testcontainer')\n",
    "\n",
    "\n",
    "    logger.info('START OF HORRORS OF THE BLOB')\n",
    "    horrorsof = TheBlob()\n",
    "    horrorsof.load_config() # lazy loading -> only done when needed\n",
    "    horrorsof.initialize_client()\n",
    "\n",
    "    logger.info('UPLOAD BLOB INSTANCE RUNNING')\n",
    "    horrorsof.upload_blobs()\n",
    "\n",
    "    logger.info('DELETING BLOB INSTANCE RUNNING')\n",
    "    horrorsof.delete_blob('testcontainer', 'test.csv')\n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Instructions\n",
    "\n",
    "1. An Azure account with an active subscription. \n",
    "2. A Batch account with a linked Azure Storage account.\n",
    "3. A Data Factory instance. \n",
    "4. Batch Explorer downloaded and installed.\n",
    "5. Storage Explorer downloaded and installed\n",
    "    **not necessary \n",
    "6. Python 3.7 or above, with the azure-storage-blob package installed by using pip.\n",
    "    **When using Batch EXplorer gives option to set up Batch pool fro 'Data Science' projects that come with python and packages for data science\n",
    "    **I did not see that option using Azure portal alone\n",
    "7. Prepare requirements.txt file (most packages are not required with Data Science batch pool but I did it as an exercise)\n",
    "    1. pip install azure-mgmt-storage\n",
    "    2. pip install azure-storage-blob\n",
    "8. Prepare json file with parameters to use in python script like connection string etc\n",
    "    **You can also use ADF pipeline Parameter list OR Batch Process Extended Properties"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features of this pipeline\n",
    "1. The pipeline is set around csv file type. It will throw error for any other unless requisite changes are made in all ADF blocks\n",
    "2. Data ( csv files), python scripts and all dependent scripts are placed in one blob\n",
    "3. As data is copied to the Batch pool, it is quoted in the process and names of files with white spaces suffer. **method rename_files was an attempt to fix this. It does not work. it is in the code set for sake of capturing entirety of my work\n",
    "4. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "1. https://pypi.org/project/azure-storage-blob/\n",
    "2. https://learn.microsoft.com/en-us/azure/data-factory/connector-file-system?tabs=data-factory#sample-linked-service-and-dataset-definitions\n",
    "3. https://learn.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobserviceclient?view=azure-python ***\n",
    "4. https://learn.microsoft.com/en-us/azure/batch/tutorial-run-python-batch-azure-data-factory ***\n",
    "5. https://stackoverflow.com/questions/52014916/enumerate-blob-names-in-azure-data-factory-v2 *****\n",
    "6. https://azure.microsoft.com/mediahandler/files/resourcefiles/azure-data-factory-passing-parameters/Azure%20data%20Factory-Whitepaper-PassingParameters.pdf **\n",
    "7. https://learn.microsoft.com/en-us/azure/data-factory/ *****\n",
    "8. https://www.cathrinewilhelmsen.net/series/beginners-guide-azure-data-factory/ *****"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EnvMotorcade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
